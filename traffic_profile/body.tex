\chapter{Wide Area Network Based Data Center Energy Simulations for Internet Services}
\label{chp:traffic}

\section{Chapter Abstract}
    Data centers are a critical component of modern day economies and social lives of people all around the world. In many aspects society’s dependency on internet services enabled by data centers is still increasing; the COVID-19 stay in place orders are a current example of unprecedented growth rates for many internet services. With the increased dependency on data centers, there is a concern about their environmental impacts across at the globe scale. To reason about the environmental footprint of these globally spanning system requires a new approach to model building energy use of facilities within these systems. This paper demonstrates a top down process that leverages publicly available network traffic data for a globally distributed internet site to model it’s power demand, using Wikipedia as the model internet site. The traffic provides an indication of the building loads that the data center infrastructure must support.  
    
    This chapter demonstrates a method to geographically correlate five data centers with the incoming visits from a global user base using a minimum cost function. The correlations are further extended to provide a forecast of the traffic. The contribution of this research is two-fold. First, it presents a method for a top down assessment of network traffic that can characterize the information technology loads in building energy models. Second, the traffic forecast allows data center operators and designers to model future demands of the building infrastructure systems and perform scenario models based on the projected traffic. 

\section{Introduction}

    Data centers are ubiquitous in today’s society. Consumers of data center enabled web services expect high availability of their functionality and access to their data on a continual basis. These expectations are met with software architectures that provide strongly consistent views of the data regardless of location and in the presence of physical systems failures. In terms of specific failure domains, network failures can be the most catastrophic. For example, an outage of a network link connecting a data center can lead to all physical resources in it to become inaccessible from the outside. With these expectations, the dependencies of internet data centers and their communications network during failures is clearly apparent.
    
    Data center utilities (such as mechanical cooling resources and power distribution resources) and their network connections are not only coupled in failure scenarios described above, but they are also in lock-step with each other through normal operations. This coupling is also easy to reason about; as internet data centers don’t generate workloads hermetically. The workload demands for data centers come from remote consumers (user facing traffic) and other adjacent data centers (back-end traffic). Both forms of incoming traffic instantiate computational and storage processes on the IT hardware, which in turn are powered and cooled by district scale distribution plants.
    
    In the current generation of data centers, the building utility systems demand less than 10\% of the total energy delivered to the site \cite{Shehabi16}. The balance of the power of these new data centers is consumed by the information technology (IT) equipment. With this disproportionate allocation of energy, it is clear that the total energy use in data centers is more sensitive to ITE loads than the facilities systems. Yet the building systems still receive the most attention in the design and facilities operations phase of the buildings. Others have also pointed out similar gaps data center building energy models \cite{Beatty15}. 
    
    To provide an intuition of data center networks, Figure~\ref{net_diag} shows a generalized depiction their topology. At the top are the internet service provider (ISP) networks are shown as clouds because the physical paths of these systems are managed by the global ISPs and they are typically out of control of data center operators. Data center operator’s depend on the ISPs for connecting their facilities through private wide area networks (WAN). The pink lines indicate these wide area connections that traverse over ISP networks. The magenta links connect global WANs data center metropolitan, which typically are responsible for the last miles of network distribution. Within the metropolitan area, their maybe one or more data center sites, only two are shown for clarity in the figure. 
    
    \input{traffic_profile/content/generic_netdia}
    
    To motivate business incentives  for  a  more  integrated  IT  and  building model, the  data  center construction spending is estimated to be \$89 billion by 2027, nearly doubling the spend seen in 2018 \cite{dcmarket19}. At the core,the demands for data centers is driven by the expected consumer traffic to its services. With this much value at stake, capital and operational decisions must be based on robust system level models that couple IT traffic and data center designs.
    
\section{Background}
    EnergyPlus, the US Department of Energy’s building energy modeling software has supported data center model since release 8.3. [4]. EnergyPlus modeling engine allows for IT equipment load schedules to be explicitly modeled in it. Today, EnergyPlus comes with three example data center models, in the form of IDF files. These IDFs provide a template that energy analysts can extend and augment to represent their specific data center designs. These default templates are hermetic and lack coordination with the dynamic IT loads. However, there are supplemental software applications that allow external software to integrate with EnergyPlus yielding a robust feedback loop. Building Control Virtual test bed (BCVTB) is one such application \cite{EnergyPlus8.3}. 
    
    \input{traffic_profile/content/agent_bem}
    
    Wei demonstrates the use of BCVBT in \cite{wei17} for modeling a mixed used facility and has a data center collocated with occupied spaces. Zhang demonstrated the use of BCVBT for control an educational building \cite{zhang19}. Zhang published their Python code base to allow others to replicate their work. Functionally, BCVTB creates an interactive process loop with EnergyPlus allowing the Python program to be aware of EnergyPlus simulations at every simulation time step. Figure 2 illustrates this interactive loop between EnergyPlus and Python. Next, background about wide area network from an internet service perspective is provided. 
    
    Wide area networks serve as the backbone for many internet applications that operate across regionally distributed data centers. The traffic on the inter data center links are segregated into three classes; interactive, large file transfers, and background. Linkedin’s experience with these three classes of traffic is document well by Zhuang \cite{zhuang15}. Interactive traffic consists of blocking operations; where the sequential byte level transactions are required to keep things proceeding. Large file transfers entail transmission of requested resources by some deadline. Background traffic are workloads that opportunistically send traffic and preempting them does not expose the system to any idempotent side effects. Database communications traffic typically consists of workloads that are bound by deadlines and are also pseudo-interactive. These three classes of traffic can either be user facing or back-end. Microsoft and Google have both published their experiences in designing and operating these massive global networks \cite{hong13, sushant13}.  

\section{Methodology} 
    In this section the methods for synthesizing network traffic to correlate it with data center workloads is described. There are four step-wise methods that ultimately yield results that are usable for building energy modeling. These sub-sections first describe the service level data-set that is used throughout this article. Then the service level information is abstracted to indicate network bandwidth, which in turn is correlated with power in the third sub-section. The final method discussed shows a method to scale historical service demands into projection of future loads for data centers.  
    
    \input{traffic_profile/content/traffic_profiles}

    Traffic to 145,063 Wikipedia Pages for 803 days from July 2015 through September 2017 is used to demonstrate a top-down WAN model for global services. The data is used to train and test the models developed in this section. The training set is partitioned to be inclusive of traffic from July 2015 to December 2016. The remaining data is used to test accuracy of the models. All pages are segregated into 7 different languages; namely English, Japanese, German, French, Chinese, Russian, and Spanish. The mean volume of visits per language is shown in \ref{mean} and the 95th quantile is shown in \ref{95thquantile}.  
    
    The curves in Figure 3 are indicative of the distribution of visits for each of the seven languages found in the data set.  For example, the 95th quantile is indicating that 95 percent of the pages in the language were visited as noted on the y-axis on the day number noted on the x-axis. While the average curves indicate that on average, the number pages in the language are visited as noted in the y-axis on the corresponding day count. Both of these curves can be used in the design phase of the data center buildings, with the 95 quantile determining the capacity and the average values supporting operational models. In this article, the average value is used as the indicator of the operational network model. 
    
    \input{embodied_cost_model/contents/world_language_map}
    
    After segregation of the pages by language and dropping pages without language indicators, the languages are mapped to countries in which they are the official language. Figure 4 shows the mapping of languages to their respective countries by color groups; English (blue), Japanese (navy blue), German (gray), French (red), Chinese (light green), Russian (forest green), and Spanish (cyan). Red markers indicate the locations of Wikipedia data centers from around the world as listed in Table~\ref{language_to_dc}.
    
    \input{traffic_profile/content/lang_to_dc_from_csv}
    
    Throughout this research these Wikipedia data centers are used to the model the network based on publicly available information. Several additional assumptions are made in regard to the attributes of the system that are not in the public domain. Furthermore, some of the system complexity is reduced for brevity. For example, the traffic generating in a particular country is allocated with their closest ingress points as listed in Table~\ref{language_to_dc} by using a minimum distance function between the respective geographical codes. 
    
    The specific step-wise approach of mapping languages to the respective data center is shown in Table~\ref{traffic_allocation_steps}. Following these steps, Table~\ref{language_to_dc} shows that English is the most dominant language, with its traces appearing at all of Wikipedia's ingress sites while originating in 49 countries.  
    
    \input{traffic_profile/content/traffic_allocation_steps}
    
    \subsection{Scaling service demands and data center workloads}
    
    In this section, the Wikipedia traffic profiles are used to forecast future demands. Note that the traffic data is slightly modified to represent digital bits in order to demonstrate another useful insight that network traffic inherently provides. The objective here is to determine the future service demands that drive workloads at each data center. To obtain the digital payload of the pages, their contents are converted to its byte size representation. The bit size distribution of Wikipedia articles was not readily available in the public domain so an estimation method was developed for this work as follows. First, several resources were consulted to determine the page size \cite{wiki_stats, xtools}. The resulting page sizes for each of the seven languages studied in this article are noted in Table~\ref{language_to_page_size}. 
    
    \input{traffic_profile/content/language_to_page_size}    
    
    Now, the time series of bit-wise representation of the traffic can be analyzed to make future predictions. The time series ($t$) dependencies of a variable $X$ can generally be decomposed in trending ($T$), seasonal ($S$), and random ($R$) components, as shown in equation below \cite{zhuang15}. Specifically T exists when there is a gradual shift across time. indicates cyclic patterns, and R accounts for randomness.   
    
        \begin{equation}
        X_t = T_t + S_t + R_t
        \end{equation}
    
    In the next few paragraphs two time-series forecasting frameworks that decompose time series into these three components are discussed. First, the Auto Regressive Integrated Moving Average (ARIMA) model is presented and its implementation challenges along with its results are noted. Second, as alternate to ARIMA, Prophet another time-series modeling tool is implemented and its details along with its results are presented. 
    
    \emph{ARIMA Model}: First, time-series analysis is performed using the Auto Regressive Integrated Moving Average (ARIMA) model. The auto regression component leverages the dependencies between an observation and some lagged time step. The integration component differentiates the raw observations from preceding time steps to make the series stationary. The moving average component quantifies the dependency between observations and residual errors with a moving average model applied to lagged observations. Each of the three components are specified as a model input parameter in the form ARIMA(p, d, q). The parameter p indicates the lag order, d indicates the degree of difference (i.e. the number times raw observations are differenced), and q indicates the size of the moving window (the moving average). 
    
    The ARIMA model has been used effectively in previous literature to forecast 21 days of network traffic with 42 days of past trends [8]. This research extends the forecast out to 9 months. Figure 5’s legend indicates the use of combination of ARIMA(2,1,2) and ARIMA(1,2,4) as parametric arguments to the ARIMA module. The generated estimate is based on 550 historical records for each site and forecasts the traffic profile 250 days beyond the training data cutoff date. Figure 5 shows that the ARIMA predictions are heavily influenced by the trend near the training cutoff period. As shown, the ARIMA models appear to capture weekly seasonality well but fail accurately capture longer seasonality periods; given the time range of the our dataset with the parametric used in this work. Nonetheless, ARIMA models are capable of including seasonal covariates to improve sensitivity to longer cyclic periods, but the attempts to model with Seasonal Auto-ARIMA led to extremely long fitting times at best or the models didn't converge on a solution. 
    
    \input{traffic_profile/content/arima}
    
    \emph{Generalized Additive Model}: Next, a variant of the generalized additive model (GAM) was simulated with the same bit wise time series from above. The model is explained in great detail by Taylor in \cite{fbprophet}. The GAM model variant, Prophet, also has a decomposable time series formulation similar to the time series dependence equation from above. However, Prophet is supplemented with a term that accounts for holidays that occur at idiosyncratic  intervals. 
    
    Prophet has a novel concept that deals with trends in forecast uncertainty, in that it uses a Laplace transform to randomly distribute future change points in the trend based on past data. This can be seen by how closely the forecasts, blue lines track the historical data, black dots in Figures~\ref{DFW}, \ref{AMS}, \ref{SFO}, \ref{SIN} and the actual forecast based on the test data (magenta lines in figure). The change point parameter is set to 0.3 and the yearly seasonality parameter is set to True for these simulations. The forecast extends beyond the test data range which shows the that the cone of uncertainty increases as the forecast is extended out further indicated by the light blue curves. 
    
    
    
    \input{traffic_profile/content/prophet_curves}
    % Provide a compartive table for MAPE of Arima and Prophet
    
    Table~\ref{mape} provides a comparison of the mean absolute error (MAPE) for four data center sites. The difference in error rates varies by a factor 1.72 (for SIN) and over 52 (for SFO). Overall, in comparison it is shown that the Prophet model is superior to the ARIMA model in terms of accuracy of its predication's. 
    \input{traffic_profile/content/mape_from_csv}
    
\section {Results}

    In this section the results from using the traffic coefficients is presented. Following the steps listed in Table~\ref{traffic_allocation_steps} yields the curves indicated in Figure \ref{ingress_hitrate_95_links}. Each curve in the figure consists of all the languages that are routed to it, truncated to the . 
    
    To reason about these curves, recollect the distribution of the languages at each data center form Figure 1. For example, the curve for San Francisco (SFO) in Figure 7 is nearly 100 times smaller than the curve for the Haarlem (AMS) site. This is due to only four countries being routed to San Francisco, whereas Haarlem has traffic routed from 38 countries. 
    
    Figure 8 shows the resulting CPU power profiles when using traffic coefficients in EnergyPlus simulations. The average page view values are used as indicated in Figure 3A for the English language. The details of the EnergyPlus configuration are provided by Kumar in [14]. Since the English language is the dominate language at the San Francisco site nearly all of the CPU resources at the site are dedicated to it. The constant load profile for San Francisco is constant in the figure and can be attributed to the nearly flat curve for San Francisco shown in Figure 7. 
    
    \input{traffic_profile/content/ingress_curves}