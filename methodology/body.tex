\chapter{Background and Overview of End to End Modeling Methods}
\label{chp:background}

\section{Chapter Abstract}

This chapter presents the methods used to identify the scope of this dissertation's research. These are the methods used to understand and address research questions to quantify the total cost of hyper-scale internet data center building infrastructure. This chapter first reviews the state of the data center industry and makes a case for making them operationally sustainable for business and the environment. Then, similar works that inspire the research hypothesis are reviewed in detail. The review is followed by statements of the research hypothesis and an overview of the dissertation structure. The chapter then provides a detailed overview of the authors understanding of the physical infrastructure that allows for a simple, yet comprehensive end to end model of globally distributed data centers.  In subsequent chapters the presented framework is proven to be capable of quantifying the end to end costs of a global system using life cycle analysis methods by demonstrating its effectiveness with the quantification of $CO_2$ emissions. 

\section{Introduction}
\label{sec: introduction}
    This research considers energy and $CO_2$ inventories across the life time of hyper-scale data centers (DC). These DCs house scalable computational architectures consisting of buildings, cooling plants, power distribution systems, compute hardware, digital storage hardware, and network hardware which are the status-quo for internet service operations. Physically, the size of individual internet DC facilities have increased from modest footprints to now span millions of square feet over the last decade; coinciding with ubiquitous penetration of the internet into the lives of people around the world . The increase in demand for DCs motivates this work as a tool kit in the DC building infrastructure design process. Beyond, the building design process, this kit is intended to be an element for broader business models for internet products.
    
    For internet products, DCs represent a major fraction of their total costs of operations and environmental footprints.  This work develops a modeling framework that couples building energy models, marginal cost of grid energy models, and the embodied costs of materials to provide an end to end assessment of data centers. Specifically, this dissertation demonstrates the models by quantifying the $CO_2$ emissions as a singular metric that is analogous to monetary costs.
    
    Next in Section~\ref{sec: background}, contextual background about the sustainability of data centers is presented. The background indicates the research motivation behind the life cycle perspective for data center systems. Then in Section~\ref{sec: similar works}, similar works that have analyzed data center life cycle costs are reviewed. These works guide the development of the resulting models from this research by pointing to low hanging fruits and gaps in the current practices. To address the low hanging fruits and fill the gaps, the research hypothesis is stated in Section~\ref{sec: hypothesis} with the overall structure of the dissertation presented in Section~\ref{sec: structure}. In Section~\ref{sec: data source}, the limitations of the primary data sources are described. The chapter then provides an overview of this research's understanding of data centers in Section~\ref{sec: overview}. This understanding of DCs allows coupling of network models with coarse grained representations of the information technology (IT) and building systems.  Section~\ref{sec: tieing it together} culminates the various IT and building components from Section~\ref{sec: overview} into deployable units. Section~\ref{sec: conclusion} concludes the the chapter by summarizing it. 

\section{Background}
\label{sec: background}

    Data centers (DCs) will consume 13\% of the world's energy production by 2030 according to worst case prediction models \cite{andrae15}. This high energy demand is not surprising given the ubiquitous penetration of the internet nowadays. However by factoring technology innovations, some models show a brighter picture. This picture is aligned with current trends, where efficiency across the entire DC stack indicates a downward trend for power demand per unit of performance. As an example, by 2016 the throughput of workloads had increased 350\% since 2012 with the same power demand \cite{GoogleEnvRpt}.  The hardware efficiency gains seen over the last decade still do not offset the need for additional DC physical capacity.
    
    Internet DC capacity is not bound by physical sizes or geographic locations however. Location agnostic networks spanning the globe allow IT systems to scale in resource parameters such as CPU, GPU, memory, storage, and data access rates. Parametric scaling of these resources enables operational tuning to compensate for physical capacity constraints. Capacity fungibility of DC parameters at a global scale leads businesses with risky practices to provision building scale data-center infrastructure very lean, while businesses that are adverse to risk can be excessively conservative. In the case of lean provisioning, all physical resources are highly utilized with little or no headroom. For the conservative approaches, normal operating conditions are such that sustained operational points are well below the capacity of the equipment. Operations significantly below capacity allows the service to accept bursts in usage, inorganic growth, and be fault tolerant in case of failures elsewhere in the system. 
    
    Lean and conservative operations are both classified as equivalent based on today's sustainability indicators and both have strategic advantages for data center owners. For example as a case for conservative deployment consider the abrupt change in internet traffic for many communications, media, and collaboration web sites due to COVID. The excursion in traffic has critical dependencies on data center facility infrastructure.  Traffic pattern changes for a set of services are shown in Figure~\ref{img_nyt_covid}. In this case if the data centers were operationally loaded to peak capacity then it would be impossible to absorb such drastic changes. However, conservative provisioning has a profound business trade-off in that it may leave resources stranded for a majority of their lives. 
    
    \input{methodology/content/nyt}
    
    In terms of capacity, DC facilities can be measured with multiple attributes; power limits, cooling limits, network limits, and floor space. Out of these, power has shown to be the most prevalent indicator of DC capacity, \cite{barroso18}. However, the other attributes don't linearly scale with power. For example, the researcher designed a data center in mid 2013 - sizing the facility on historical trends. By the time the data center was deployed the power density per information technology devices had significantly increased. With the increase in the power density, more than 60\% of the designed floor space was left stranded (ie not deployable) as the facilities power and cooling capacity limits were reached at 40\% of the designed floor area. However, if the DC design team had looked closer at Moore's law such disruptive change in density were apparent. Figure~\ref{img_moores} indicates the log-scale nature of de-materialization of transistor based technology made famous by Moore.
    
    \input{methodology/content/transistor}
    
    Operationally, the prevalent indicator for sustainability of DC facilities is the Power Usage Effectiveness metric ($PUE$, see equation \ref{eq:pue}). $PUE$ is a dimensionless ratio of load vs. supply energy.  In Equation~\ref{eq:pue}, $E_{IT}$ indicates the IT energy load and $E_{total}$ indicates the total energy supplied to the facility. The $PUE$ is generally reported on a quarterly or annual basis by integrating the equation across the respective time period. For a given period, the metric measures the operational efficiency of the facility's system compared to the information technology equipment (ITE). Spaces housing ITE have seen profound cooling and power distribution efficiency gains resulting from the focus on $PUE$, however the metric does not comprehensively cover sustainability of DCs.
    
    \input{methodology/content/equation_pue}
    
    The $PUE$'s shortcomings as a sustainability metric are noted by Horner, particularly that low PUEs do not correlate to low carbon footprint \cite{Horner16a}. The tendency of carbon intensity to vary based on energy source characteristics is demonstrated by Masanet in \cite{Masanet13a}. Masanet shows that energy supply sources must be coupled with lowered demands to be good indicators of use-phase carbon footprint. For example, the carbon footprint of a low $PUE$ DC with it's energy sourced from a coal plant will be higher compared to a DC that operates with a slightly higher $PUE$ but sources power from a renewable source such as wind. This is summarized in Figure~\ref{masanet13a1}.
    
    \input{methodology/content/pue_not_sustainable}
    
   From positive point of view, using $PUE$ as the de-facto energy efficiency metric led ASHRAE to recognize the significance of the cooling systems to the data center energy use. In rapid response they developed guidelines to help operators optimize their cooling systems. One of the most profound contribution from ASHRAE has been their position on expanded thermal boundaries for the data center environments, as indicated by the black boundaries in the pyschrometric chart shown in Figure~\ref{psychrometric}. The expansion is relative to a static cooling set-point temperature of $55^{\circ}$F that was standard practice for legacy operations. The expanded thermal boundaries have an influence on nearly all elements of a data center facility during their design phase, but most importantly ASHRAE's guidelines for the expanded thermal boundaries has helped facilities operators configure their cooling plants to minimize the energy use and lower their $PUE$.
    
    \input{methodology/content/psychrometrics}
    
    A facility's thermal environment directly couples the IT and building systems. The first order impacts of this coupling are on the cooling system; spanning from the cooling tower to the node junctions for the nano-scale transistors. The end to end flow of the heat rejection is shown in Figure~\ref{img_dc_infrastructure}. 
    
    \input{methodology/content/dc_cooling}
    
    However, there are also many second order effects that must be taken into account when developing disruptive designs that deviate from the heat rejection paths shown in Figure~\ref{img_dc_infrastructure}. When deviating from these legacy practices, trade-offs across the entire life cycle of the system must be characterised. As an example, the researcher's need for a life cycle analysis framework was recognized during the development of a novel bottom-cooled hermetic rack \cite{gao16}. The test chamber for the cooling coil and the computational fluid dynamic model of the bottom-cooled rack is shown in \ref{bcu_bench} and \ref{bcu_rack}, respectively. This development alone required a deep trade-off analysis of various systems that could be most appropriately be done with a life cycle analysis framework. 
    
    \input{methodology/content/bcu_design}
    
    Specifically for the bottom cooling unit; the second order trade-offs included the changes to the building systems, it's impact to the network topology, chiller plant augments to support revised thermal requirements, and power distribution changes due to changes in the rack power densities. These trade-offs required awareness of the data center service level agreements as not to strand space, power, or network ports. Furthermore, the business evaluation of such a development effort had to consider all of the above factors to indicate cost vs. performance for comparison with alternate technologies as shown in Figure~\ref{img_bcu_second_order}.
    
    \input{methodology/content/second_order_bcu}
    
    Beyond the operations phase of DCs, the IT hardware and physical infrastructure products they're composed of have many other life-cycle phases. At each life cycle phase of the products, there are energy and $CO_2$ inventories. With life cycle analysis (LCA) principles these inventories can be tallied from their raw form to their useful state.  The LCA perspective allows the treatment of these inventories as either a debt or credit to it's environmental footprint. The debt and credit approach follows a structure analogous to total costs of ownership (TCO) methods used in today's sophisticated business models for DCs \cite{Hardy12}. These modern business models amortize capital and operating costs over a system's useful life similar to LCA. This research therefore threats $CO_2$ as analogous to monetary cost and demonstrates the end to end $CO_2$ inventories in this dissertation.
    
    The direct effects described so far and discussed throughout this work are the most transparent impacts associated with information communication technology (ICT). The indirect or higher order impacts fundamentally alter the use of energy in a wide breadth of applications. Figure~\ref{ICT_econ} shows examples of the breadth ICT impacts to other industrial sectors. However, this research followed the approach set by The Green Grid where the DC is a physical entity of interest and focuses on the assessment of it's direct life cycle activities \cite{tgg12}. The direct activities are indicated by the red enlarged area shown in figure \ref{fig:f2} which sets the boundary conditions described later. 
    
    \input{methodology/content/img_ict_higher_order_imacts}
    
\section{Similar Works}
 \label{sec: similar works}  
    Now, pioneering works that have quantified the life cycle costs of DCs are presented. 
    
    The most explicit DC life cycle cost assessment is reported by Whitehead \cite{whitehead15}. Whitehead proposes guidelines and criterion for DC life cycle assessments in-line with International Standards Organization (ISO) 14040 \cite{ISO14040}. Whitehead's report is reflects the LCA of a UK DC with legacy infrastructure. Using the LCA results of the UK DC as the benchmark, they evaluate a Swedish DC with state of the art infrastructure and a much higher server stock turnover (refresh) cycle. The boundary conditions and life cycle phases of both LCAs are shown in figure \ref{LCAphases}. Whitehead finds that refresh cycles have the most significant contribution to the life cycle impacts in today's state of the art DCs compared to legacy DCs where the operational energy dominate. In this dissertation, Whiteheads functional-unit of \textit{1 kW of IT per year in a Tier III facility} is adapted as a reasonable unit of measure of data center functionality.
    
    \input{methodology/content/whitehead_boundaries}
    
    The Hewlett Packard Corporation published a series of papers around the sustainability of information technology sector in the early 2010's. Two of the most relevant to data centers were led by Shah and Chang \cite{shah11, shah12}. In the first work, Shah demonstrated the use of the economic input-output (EIO) model for assessing the embodied costs of the major infrastructure categories found in data centers (see Table \ref{shah_components}). Their development of the environmental model for the data center was ultimately a combination of process based and EIO methods. Building on Shahs's EIO based framework, Chang uses the thermodynamic metric of exergy to quantify data center sustainability. By quantifying the irreversible thermodynamic processes in terms of exergy allows normalization in terms of architectural parameters \cite{shah12}. A parametric view enables systems designers to reason about only an intuitive set of constraints.
    
    \input{methodology/content/shah_boundaries}
    
    In addition to the independent LCA perspective of Whitehead and the corporate perspective of Hewlett Packard, the Department of Energy's Lawrence Berkeley National Lab has produced the CLEER online modeling tool \cite{CLEER13}. CLEER is a web based user interface that allows consumers to model the migration of on-premise software workloads to cloud based data center platforms. As shown in Figure~\ref{img_cleer}, the charter for the CLEER model is to consider the end to end energy associated with cloud services.
    
    \input{methodology/content/cleer}
    
    In the subsequent chapters, this research builds upon these pioneering similar works described above. Six parameters common to all three are identified and supplemented with novel contributions developed by this dissertation research. Two additional parameters are also added by this dissertation's end to end modeling framework. Namely, the framework presented in this work are also inclusive of network traffic and building operations as shown in Figure~\ref{img_gap_matrix}.
    
    \input{methodology/content/gap_matrix}
    
    The background discussed in this section inspire the research question of this dissertation. Namely the research question is \emph{how can DC designers make decisions that have longevity with all of the uncertainty involved?} Next in Section~\ref{sec: hypothesis}, the research hypothesis is stated followed by an overview of the dissertation's structure that validate the hypothesis in Section~\ref{sec: structure}.
    
\section{Hypothesis}
\label{sec: hypothesis}

    \emph{\large To make effective DC design capacity decisions, the decisions must be based on scalable and agile models.}
        
    Sub hypothesis are as follows:
    
    \begin{enumerate}
        \item The model needs to assess end to end cost trade-offs and be technology agnostic.
    
        \item Monetary cost models for DCs are analogous to environmental cost models. LCA modeling community has proven its effectiveness for analysing global scale systems and supply chains.
    \end{enumerate}
    
\section{Structure of Dissertation}
\label{sec: structure}
    This research is composed of four modules that culminate academic experiments and analysis with extensive professional experience in designing, planning, building, and deploying data center systems through out the world. The modules are presented in this dissertation as independent, but correlated chapters. The segmentation of the chapters roughly aligns with the software modules developed as shown in Figure~\ref{process_flow}.
    
    \input{methodology/content/software_processes}
    
\section{Note on Data Sources}
\label{sec: data source}
    The underlying data sources that construct and validate the developed software modules are sourced from publicly available references and professional heuristics. Generally, the preference of this research is to use publicly available data for academic posterity, however there are several critical pieces of information that can not be publicly disclosed. In these cases, where no relevant public information was found, the author leans on professional heuristics. 
    
    Due to the nature of the technology business, most of the heuristically derived data points are protected under non-disclosure agreements. Every effort is made not to impede these agreements.

\section{Overview of Data Center Systems.}
\label{sec: overview}
    The research's understanding of data centers and the various systems with-in them are now presented. The first set of systems addressed are the IT equipment. The IT equipment are viewed as an ensemble at a building or cluster level (see Figure~\ref{img_cluster_storage_hierarchy}). Viewing data centers as an ensemble of IT equipment allows for a simpler model while aligning with the single coherent views that are objectives of services that are supported in these clusters. From a service perspective, the physical implementation is agnostic of whether the functional components are aggregated within a chassis or the components are dis-aggregated across chassis provisioned with homogeneous components. In the later case, high speed local area networks are critical for orchestration of service execution. 
    
    The second set of systems addressed are the data center communication networks. As described above networks are a critical component at the core of hyper-scale data centers. It's the network that allows data center services to appear to its users as a single coherent system whether the physical components are located in the same building or they are globally dispersed \cite{tanenbaum}.
    
    The third system addressed in this section are the building systems. The building systems are the key focus of this research and this section aims to identify the key parameters that make data center facilities distinguishable from other building types. Using the combination of distinguishable parameters and a baseline building sector from the USEEIO \cite{yang17}, the baseline costs input vector (Y-vector) is presented.
    
    The following subsections provide overviews of the IT equipment, communications networks, and buildings as they are reasoned about in this research.
    
        \subsubsection{Information Technology Equipment (IT)}
            The modeling framework developed in this research is founded upon the IT equipment housed inside data centers. Table~\ref{computer_parts} indicates the data used in this research to characterize the IT equipment, which are inputs to building energy simulations and embodied materials models. The computation and storage IT equipment (servers) profiles are based on production server configurations of a large internet operator (under NDA), representing their deployments in 2015 and 2016. However, the objective of the research is to have the agility to model allocation of any component found in data centers. In that light, some of the current approaches for allocating servers and their components are described next.
            
            Servers are generally categorized as compute nodes or storage nodes. Compute servers may be fitted with state-of-the-art processors, input and output devices, and dynamic random access memory (DRAM). The latter category of storage servers are optimized for high density storage with minimal processor capabilities with input output devices selected based on the desired performance. Figure~\ref{img_server} shows air cooled server units fitted with a combination of processors and DRAM, while Figure~\ref{img_storage_tray} shows a homogeneous arrangement of storage drives in a server. 
             \input{methodology/content/server_image}
            \input{methodology/content/storage_tray}
            
            Segregating the storage from the compute instances have several advantage for the software architecture; for building systems designers the most intuitive is failure isolation and operational diversity. For consideration of failure isolation, the outage of a single compute instance will also result in the stored data becoming unavailable. In terms of diversity, when compute devices and storage devices are on the same physical chassis, a single compute processing device may not be capable of handling all the requests to access the stored data within performance bounds. Separating the data into different servers allows the incoming read or write requests for the stored data to be balanced across a set of processors based on performance bounds. This relationship is the premise of many modern data center cluster designs, where stored data and compute resources are strategically arranged. Figure~\ref{img_cluster_storage_hierarchy} indicates such a cluster storage hierarchical scheme from Barroso \cite{barroso18}. 
            
            \input{methodology/content/cluster_storage_hierarchy}
            
            In this work, servers with a combination of compute and storage components are characterized for power as listed in Table~\ref{computer_parts}. The power demands of these components are obtained using the individual power requirement for each component from Table~\ref{tab:it_component_power_dist_table}. The combination of component counts and individual power values together quantify the total power of the servers. In the servers studied, their total power demand range from 320 watts to 334 watts (see Total Power row in Table~\ref{computer_parts}). These heterogeneous server choices are used in the models for three reasons:
            
            \input{methodology/content/computer_parts} 
            
            \begin{enumerate}
               \item The research had first hand access to representative cost data for these servers under NDA. 
               \item Mixed component server assemblies can properly characterize the embodied materials of the overall DC by linearly scaling the power or count of the components.
               \item Mixed server types allow the components to be quantified by packing the fractional power demand of each component with-in the server into the total power of the DC. The method for packing various component types into the DC is indicated in Algorithm~\ref{it_y_vector_algo} (see IT discussion in Section~\ref{sec: embodied}).
            \end{enumerate}
             Using the heterogeneous servers captures all of the components that influence the developed life cycle cost models. Finer resolution is not sought as the models that are developed in this research don't account for the airflow patterns or the IT rack layout inside the buildings. Nonetheless, the fidelity of the framework can be enhanced for quantifying the operational energy if the physical layouts of the servers are more precisely characterised.
            
            %  Examples of the design choices for the server components arrangements and building layouts are now discussed. These examples point to modeling improvements that can make this framework more precise. Furthermore, these example are used to normalize the servers into their deploy-able units; a server rack.
            
            The servers discussed above are housed in steel racks as shown in Figure~\ref{bcu_rack} and Figure~\ref{goog_rack}. At the rack level, collocated servers can communicate over switched network links within the rack (see Figure~\ref{img_server_rack}).  Racks can also communicate with other racks, bridged by high speed communications networks across local areas (shown as network Ingress/Egress in Figure~\ref{img_server_rack})) . Communicating racks are constrained by the software application's latency bounds. The communication latency values (a measure of communication delay) are attributes of the optical instruments and cabling system as discussed in the Network part of Section~\ref{Network}. 
            
            The choice for the hardware arrangement is reasoned about as performance objectives for specific workloads that the rack will be susceptible to. In the end whether at the individual server, rack level, or cluster level, compute and storage devices need to communicate with each other as shown in Figure~\ref{img_cluster_storage_hierarchy}. Specific design choices must optimize the service level performance objective in terms of processing speed, data volume, read/write latency, and the read/write rates. Performance optimization can by characterized by analysing the system through queuing theory. Applications of queuing theory relative to computer systems is presented by Harchol-Balter in \cite{harchol13}.
            
            \input{methodology/content/rack_image}
            
            The server rack shown in Figure~\ref{goog_rack} illustrates the arrangement of local power conversion, power storage (batteries), and network devices along with the server payloads. This rack is indicative of a typical rack found inside Google's data centers as discussed by Barroso \cite{barroso13}. At the rack level, the incoming power feed is generally alternating-current (AC) voltage while all the server components require direct-current (DC) voltage. In legacy servers the AC to DC voltage conversion happened within the server, however in modern racks the power is rectified at the rack level with more efficient rectifiers that feed DC voltage to bus-bars for distribution to servers \cite{open_compute_48V}. The rectification external to the servers leads to some additional heat at the rack level in addition to the heat dissipated by the server components. In many rack configurations, all servers are connected to top of rack (TOR) switches. These TORs are the hub for all the server's communications cables. The network connections are discussed further in next subsection. 
    
        \subsubsection{Network Model}
        \label{Network}
        
        This subsection provides an overview of data center networks. This understanding of the network allows the research to abstract the building level power demands based on simple server form-factors as discussed above and motivates the network correlated building energy models presented in the subsequent chapters, where the building energy power and cooling loads are reset based on ingress network traffic to a data center building. For internet scale computing, the large data sets required simply do not fit on any single machine. At the building/cluster level, processes running on CPUs require instructional and process data that maybe stored in remote racks. Therefore local area network (LAN) fabrics between servers is critical for allowing CPUs to access remote storage devices \cite{mccullough2012enabling}. Furthermore, services operating in geographically remote buildings also require communications with each other over wide area network (WAN) for performance and availability objectives. A generic network topology is shown in Figure~\ref{net_diag}.
        
        To illustrate the communications at the rack level, a schematic server rack arrangement with it's network connections is indicated in Figure~\ref{img_server_rack}. In this arrangement, servers can communicate with other collocated servers in the rack through the top of rack switch. This is sufficient for relatively small services and is a reason to mix storage and CPU servers at the rack level. However, as discussed above it is not trivial to provision static dependencies between processes and stored data for global scale services, so distributed systems design principles must be exploited \cite{tanenbaum}. In distributed systems, external connections to and from the rack is essential for not only accessing data or processes that aren't locally available but also for things like replication and communications with the service users. 
    
        \input{methodology/content/server rack}
        
        As discussed above, when processing and storage servers are not collocated on a rack, they are bridged together with high speed communication networks. A collection of densely connected servers and racks over a communication network is commonly termed a cluster. A popular network topology within clusters is illustrated in Figure~\ref{img_fb_clos}. The physical layout of the cluster is limited by the fiber optics and cable systems. The distance limits for multi-mode fiber cables used inside DCs are indicated in Table~\ref{table_fiber_limits}. Beyond these limits communication delays and data transmission quality become impaired. 
        
        \input{methodology/content/image_fb_clos}
        
        \input{methodology/content/cable_distance}
        
        The next section presents this research's understanding of data center building systems.
        
        \subsubsection{Building Systems}
        
        In this subsection the DC facilities and the motivating factors for their design attributes are described. The building attributes that guide this research are obtained from real world hyper-scale designs. In subsequent chapters and in the developed models, the building attributes discussed here are coupled with real project cost data. Specific building designs are not explicitly disclosed in this dissertation and the cost data have been normalized across the set of real world examples based on provisioned capacity of the respective data centers. 
        
        The form factor of data center facilities are heavily influenced by the mechanical systems conveying heat from the server components to the outdoors. Figure~\ref{img_airflow} illustrates the airflow path across a data center from the point that air enters the building to it's point of exhaust from the building. In addition, its generally the mechanical system that constrains the layout of the servers inside the facilities. The mechanical constraints make it's coordination with all the IT and other utility systems a critical part of the DC design process. Figure~\ref{img_server_room_method} illustrates racks laid out in arrays of contiguous rows. The air space between the rack rows are segregated for either hot or cold air, where the front of the servers are against the cold aisle and the back of the servers are against the hot air aisles. 
        
        \input{methodology/content/airflow}
        \input{methodology/content/server_room}
    
        
        Another influential DC attribute is their power density. In the set of designs reviewed, power density ranged from 1.99kW per meter-square to 3.3kW meter-square. This range of power density requirements make data centers distinct from conventional buildings in some respects, while in other respects DCs are similar to conventional buildings. Nonetheless, the dissimilarities between DCs and conventional building prohibit environmental cost modeling of DCs directly into economic input and output (EIO) frameworks as discussed in Chapter~\ref{chp:embodied_cost_model}.
        
        To compensate for the difference between DCs and conventional buildings, Shah has demonstrated an effective method that augments the conventional buildings represented in publicly available EIO models \cite{shah11}. This research uses Shah's method, but substitutes it's legacy EIO with the 2017 United States Environmentally Extended EIO (USEEIO). The USEEIO reflects newer economic relationship between sectors and is indicative of more recently evaluated environmental costs incurred by the respective sectors per unit of monetary cost \cite{yang17}. 
        % As an input to the USEEIO model, the relative costs column in Table~\ref{table_building_costs} indicates the vector (y-vector) developed in this work. As seen in the sector column of the table, manufacturing buildings are selected as the baseline building for this research's y-vector. The other sectors indicate the dissimilarities of DC compared to manufacturing buildings with corresponding first costs spent for goods supplied by the respective sector to the DC facility shown in the relative costs column.
        
        \input{methodology/content/building_costs}
        
        Table~\ref{table_building_costs} reflects the costs of the buildings and the incremental costs of specialized systems relative to a Watt of provisioned capacity. The costs are spent during the data center construction for goods supplied by the listed sector. The relative costs representation allows the building costs and specialized systems costs to be scaled based on the total provisioned power. The manufacturing building sector is selected from the USEEIO as being the most similar to data centers and used as the baseline for the input vector into the USEEIO. All the other systems contribute incremental cost increases to the manufacturing building cost. Nonetheless, the values in the table can be adjusted to analyze specific data center designs
        
        In the next section, the above systems are tied back to the explicit interface between building attributes and IT systems.
        
    \section{Tieing it all back to deployable increments of IT equipment}
    \label{sec: tieing it together}
    
    DCs are often populated on an incremental basis. The deployment increments are typically at the resolution of a rack, as racks provide a clear interface with the building systems. For example, when the racks are independent failure domains they can be easily isolated with a dedicated electrical circuit breaker at this level. Also, a fully populated rack is much easier to reason about for capacity constraints from a power and cooling standpoint then deploying at the extreme resolution of a single chassis at a time. 
    
    On the other end of the extreme is the step-wise deployment of servers to consume the entire power capacity of a building or cluster. This extreme may lead to expensive and rapidly depreciating hardware to be under utilized as the service demands ramp up over time. The service ramp rates motivate most DC operators to deploy hardware at rack level increments described above under just in time practices. Albeit, there are exceptions that make step-wise full capacity deployments an attractive option as exemplified by the sudden increase in demand shown in Figure~\ref{img_nyt_covid}. However, even in such cases the IT manufacturing supply chain may become the bottle neck, ultimately resulting in rack level incremental deployments.
    
    Capacity constraint management is one of the most critical aspects of data center operations, as IT devices can be added, removed, and replaced at any time during the life of the facility. Deployment of partially populated racks risks capacity being stranded (i.e. headroom on the electrical circuit capacity that can not be used else where in the electrical system) or over-subscription of resources when the end state of the power demand for all the deployable rack positions are not deterministic. To reclaim the stranded power or to alleviate the over-subscription of power may require re-shuffling of racks across a data center to de-fragment the floor space. The cost of rack moves includes the down time of a rack and staffing/engineering toil and is handled with resistance in practice.
    
    In the models developed for this research, for simplicity and its purpose as a proof of concept framework, the end state of the the data centers are evaluated without regard to the increments of deployment where the servers are all packed into the power capacity of the building. The specific server configurations that are considered in this research are indicated in Table~\ref{computer_parts}. These components are packed into the power envelope per Algorithm~\ref{it_y_vector_algo} (see Chapter~\ref{chp:embodied_cost_model}) to quantify the input costs for the USEEIO. This level of aggregation may suffice for monolithic clusters. To translate the components into the count of deploy-able racks, requires additional insights about the server rack layouts in relationship to the rest of the building floor area. Algorithm~\ref{rack_power_counts} presents the methods to translate the server power density to rack counts and rack level power. 
    
    % \input{methodology/content/equation_rack_counts}
    
    \input{methodology/content/algo_rack_counts}
    
    Equation~\ref{eq:floor_per_density} indicates the IT load density of each thermal zone. However, this density is a naive distribution of the IT load over the entire floor area; while servers can only be positioned in linear rows as shown in Figure \ref{img_server_room_method}. This rack layout reduces the area available for racks. To account for the fraction of the server floor where racks can be deployed, the available area for racks can be calculated with Equation~\ref{eq:area_for_racks}. Once the area for racks is known, the quantity of racks that the server floor can support can be evaluated with Equation \ref{eq:rack_count} and the average power for each rack can be evaluated with Equation \ref{eq:rack_power_average}.
    
     In Table~\ref{table_rack_power}, power capacity, building area, server rack dimensions, and the network coefficients are input based on the specifications of a 7,500 kW DC. By using this method, the average supportable rack power is evaluated to be 6.5 kW. As discussed above, the power demand of each type of server from Table~\ref{computer_parts} ranges from 320 Watts to 334 Watts, therefore each rack can support 20-21 servers without power over subscription at the rack level.
    
    \input{methodology/content/table_rack_power}
    
    There are several other constraints that must be optimized in real world data center deployments. The optimization may include power phase balancing, network bandwidth, network ports, heat rejection rates, air flow rates, along with several other fluid mechanics considerations. Each of these must be addressed on a situational basis but they are not accounted for in the developed framework.
        
    \section{Conclusion}
    \label{sec: conclusion}
    This chapter presented the background on the current state of the data center industry and it provided a review of the inspirational work that led to the research hypothesis. The chapter also presented an overview of three technological segments found in data centers; namely IT equipment, building systems, and network. Accurately modeling these three segments is the path to high fidelity data center models. The overview in this chapter demonstrates the author's understanding of data centers and indicates the research's specific use cases and limitations across these technology segments. In the next chapter a wide area network based building energy model is presented.
    
   
    

    
    
